---
title: "HarvardX - BEER!!! - Capstone IDV Project"
author: "Chris Mann"
date: "6/17/2019"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

As part of the HarvardX Data Science Certificate program, this Individual Learner- Capstone project focuses on the creation of a recommendation system using the __Kaggle - Beer Review__ dataset (publicly available at  [Kaggle](https://www.kaggle.com/rdoume/beerreviews)).

This datset was discussed in the very interesting Strata 2017 talk: "How to hire and test for data skills: A one-size-fits-all interview kit" [Strata 2017](https://conferences.oreilly.com/strata/strata-ny-2017/public/schedule/detail/59542)


All files can be downloaded from [GitHub](https://github.com/chrismann025/HarvardX-BEER--Capstone).

The goal of this project is to utilize the tools and techniques covered in the course to develop a recommender system, which will be judged against the simplest "Guess the Mean" approach available.

As with the MovieLens 10M project, we will use the __Root Mean Squared Error (RMSE)__ on a segregated test dataset as our benchmark.

I found the MovieLens 10M project frustrating... a 10 Million record dataset is clearly beyond the capabilities of R and Caret.  Yes, I could custom compile some code to enable GPU support for some libraries, but why should I have to?  Part of the reason that I took this course was to brush up on my "R."  I thought that the language was getting a bad rap when compared to Python/Pandas or other Data Science programs...

Sadly, I was wrong.

For this individual project, I selected a dataset that is MUCH smaller hoping that I could just write an lapply statement and magically have R and Caret churn through 20-30 algorithims. Sadly, that was not the case.  The __Beer Review__ Data has approximately 1.5M records.  I did a random split of 60/40 for Training and Testing in order to ensure that the Training dataset had fewer than 1M records.

- Training: 972,788 reviews
- Testing:  613,826 reviews

Essentially, R pukes on even 1M records and the __caret__ package is literally useless unless you are fiddling with very small datasets.  So, unfortunately, R does not appear to be ready for prime time.

Given that, what is our target?  We will explore the data shortly, but the overall __rating__ average is:

- Overall Mean: 3.813921
- RMSE: 0.7159839

So our goal is to see how much better we can do than an RMSE of __0.716__

I will take several approaches:

- Caret - _Useless and no Results_
- Regularized Linear Regression
- XGB â€“ Extreme Parallel Tree Boosting (xgBoost)
- RECO - Recommender w/ Matrix Factorization (recosystem)
- Salford Predictive Modeler V8.0


```{r, echo=FALSE}

if (!require(tidyverse)) install.packages('tidyverse')
library(tidyverse)

if (!require(caret)) install.packages('caret')
library(caret)

if (!require(dplyr)) install.packages('dplyr')
library(dplyr)

if (!require(lubridate)) install.packages('lubridate')
library(lubridate)

if (!require(knitr)) install.packages('knitr')
library(knitr)

if (!require(scales)) install.packages('scales')
library(scales)

if (!require(kableExtra)) install.packages('kableExtra')
library(kableExtra)


# Global setting for the document - set cache to TRUE
opts_chunk$set(cache=TRUE,
               echo=TRUE,
               warning=FALSE,
               message=FALSE)

```


## Kaggle - Beer Reviews Dataset

### Load and Create the Train & Test Datasets

_NOTE: if using R version 3.6.0, be sure to use: set.seed(1, sample.kind = "Rounding") instead of set.seed(1)_

```{r, echo=TRUE, cache=TRUE}

# Read in the CSV file
beer <- read.csv("beer_reviews.csv")

# Drop the additional "Rating" columns and only keep the Overall rating
beer <- beer %>% select(-one_of("review_aroma", "review_appearance",
                                "review_palate", "review_taste"))
colnames(beer)

# Rename some columns to make them easier to use
colnames(beer)[3]  <- "timestamp"          # Convert "review_time" to "timestamp"
colnames(beer)[4]  <- "rating"             # Convert "review_overall" to "rating"
colnames(beer)[5]  <- "user_id"            # Convert "review_profilename" to "user_id"
colnames(beer)[9]  <- "beer_id"            # Convert "beer_beer_id" to "beer_id"

# Convert from a Reviewer Name Factor to a Numeric ID
beer$user_id <- as.numeric(beer$user_id)

# Create a numeric ID for Beer Styles
beer <- beer %>% mutate(beer_style_id = as.numeric(beer_style))

# CREATE TRAINING AND TESTING DATASETS
# The Testing Set will be 40% of the Beer Review Dataset

# NOTE: if using R version 3.6.0, use: set.seed(1, sample.kind = "Rounding") instead of set.seed(1)
# set.seed(1) # if using R 3.6.0: set.seed(1, sample.kind = "Rounding")
set.seed(1, sample.kind = "Rounding")
test_index <- createDataPartition(y = beer$rating, times = 1, p = 0.4, list = FALSE)
beer_train <- beer[-test_index,]
temp <- beer[test_index,]

# Make sure user_id and beer_id in the Testing set are also in Training set
beer_test <- temp %>% 
  semi_join(beer_train, by = "beer_id") %>%
  semi_join(beer_train, by = "user_id")

# Add rows removed from beer_test set back into beer_train set
removed <- anti_join(temp, beer_test)
beer_train <- rbind(beer_train, removed)


#******************************************************************************
#******************************************************************************

# Create working copies of the Training (beer_train) and Test (beer_test) datasets
train_set <- beer_train
test_set <- beer_test

# Define the RMSE Evaluation Function
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}

```


### Examining the Beer Data

```{r}
class(beer)
```

```{r}
glimpse(beer)
```


The __beer__ dataset is a Data Frame with 1,586,614 observations of 10 variables:

- __rating:__ User defined rating from 0 to 5 in 0.5 increments. This will be our target variable.  There are several other attributes that are rated, but they were highly correlated to the Overall Rating, so those variables were excluded.  Technically this is ordinal and could be used as categorical, but we will be treating it as a continuous variable since our evaluation method is RMSE

- __user_id__  ID of the User giving the Rating observation
- __beer_id__  ID of the Movie being Rated

- __timestamp:__ POSIX datetime INT from 1 Jan 1970
- __brewery_id__  ID of the Brewery
- __brewery_name__  Name of the Brewery
- __beer_style__  Style of Beer
- __beer_name__  Name of the Beer
- __beer_abv__  Beer Alcohol by Volume
- __beer_style_id__  ID of the Beer Style
 

```{r}
#*******************************************************************************
# Create Additional Datasets
#*******************************************************************************

# USER RATINGS - Create dataframe with additional User rating information:
#       Average (u_avg), StDev (u_std), Number of Reviews (u_numrtg)
user_data <- train_set %>%
  group_by(user_id) %>%
  summarize(u_avg = mean(rating),
#            u_std = sd(rating),
            u_numrtg = as.numeric(n()))

# BEER RATINGS - Create dataframe with additional Beer rating information:
#       Average (b_avg), StDev (b_std), Number of Reviews (b_numrtg)
beer_data <- train_set %>%
  group_by(beer_id) %>%
  summarize(b_avg = mean(rating),
#            b_std = sd(rating),
            b_numrtg = as.numeric(n()))

# BREWERY RATINGS - Create dataframe with additional Brewery rating information:
#       Average (brw_avg), StDev (brw_std), Number of Reviews (brw_numrtg)
brewery_data <- train_set %>%
  group_by(brewery_id) %>%
  summarize(brw_avg = mean(rating),
#            brw_std = sd(rating),
            brw_numrtg = as.numeric(n()))

# BEER STYLE RATINGS - Create dataframe with additional Beer Style rating information:
#       Average (s_avg), StDev (s_std), Number of Reviews (s_numrtg)
beer_style_data <- train_set %>%
  group_by(beer_style_id) %>%
  summarize(s_avg = mean(rating),
#            s_std = sd(rating),
            s_numrtg = as.numeric(n()))

# Number of unique Users
nrow(user_data)

# Number of unique Beers
nrow(beer_data)

# Number of unique Breweries
nrow(brewery_data)

# Number of unique Beer Styles
nrow(beer_style_data)

```

The dataset contains the ratings of 66,055 Beers by 33,388 Users.  There are 5,840 Breweries represented and 104 Beer Styles.


```{r}
# Plot the distribution of Ratings in the Training set

beer %>%
  ggplot(aes(x= rating)) +
  geom_histogram(binwidth = 0.25, color = "black") +
  scale_x_continuous(breaks=seq(0, 5, 0.5)) +
  scale_y_continuous(labels=comma) +
  labs(x="Beer Rating", y="# of Ratings") +
  ggtitle("Histogram of Ratings")
```

Unlike the MovieLens ratings, "half" ratings don't seem to be an issue with people reviewing beers.  It would be interesting to try and figure out why movie ratings stratify on whole numbers, but beer ratings don't.  Maybe it's just because beer drinkers are having more fun and just don't care.  ;)

User Data - Here we can see that there are a lot of Users who have only submitted one review.

```{r}
#*******************************************************************************
# USER INFORMATION
#*******************************************************************************

# Plot the distribution of Ratings by User ID in the Training set
train_set %>%
  group_by(user_id) %>%
  summarize(b_u = mean(rating)) %>%
  ggplot(aes(b_u)) +
  geom_histogram(bins = 30, color = "black") +
  labs(x="User Rating", y="# of Ratings") +
  ggtitle("Histogram of Ratings by User")

# Plot the log distribution of # of Ratings per User
user_data %>%
  ggplot(aes(x = u_numrtg)) +
  geom_histogram(bins = 100, color = "black") +
  scale_x_log10() +
  labs(x="# of Ratings (log10 scale)", y="# of Users") +
  ggtitle("Distribution - # of Ratings by User")

```

```{r}
#*******************************************************************************
# BEER INFORMATION
#*******************************************************************************

# Plot the distribution of Ratings by Beer ID in the Training set
train_set %>%
  group_by(beer_id) %>%
  summarize(b_u = mean(rating)) %>%
  ggplot(aes(b_u)) +
  geom_histogram(bins = 30, color = "black") +
  labs(x="Beer Rating", y="# of Ratings") +
  ggtitle("Histogram of Ratings by Beer")

# Plot the log distribution of # of Ratings per Beer
beer_data %>%
  ggplot(aes(x = b_numrtg)) +
  geom_histogram(bins = 50, color = "black") +
  scale_x_log10() +
  labs(x="# of Ratings (log10 scale)", y="# of Beers") +
  ggtitle("Distribution - # of Ratings by Beer")

```

```{r}
#*******************************************************************************
# BREWERY INFORMATION
#*******************************************************************************

# # of Breweries
# Top Breweries by Rating

# Plot the distribution of Ratings by Brewery ID in the Training set
train_set %>%
  group_by(brewery_id) %>%
  summarize(b_u = mean(rating)) %>%
  ggplot(aes(b_u)) +
  geom_histogram(bins = 30, color = "black") +
  labs(x="Brewery Rating", y="# of Ratings") +
  ggtitle("Histogram of Ratings by Brewery")

# Plot the log distribution of # of Ratings per Brewery
brewery_data %>%
  ggplot(aes(x = brw_numrtg)) +
  geom_histogram(bins = 50, color = "black") +
  scale_x_log10() +
  labs(x="# of Ratings (log10 scale)", y="# of Beers") +
  ggtitle("Distribution - # of Ratings by Brewery")

```

```{r}
#*******************************************************************************
# BEER STYLE INFORMATION
#*******************************************************************************

# Plot the distribution of Ratings by Beer Style in the Training set
train_set %>%
  group_by(beer_style_id) %>%
  summarize(b_u = mean(rating)) %>%
  ggplot(aes(b_u)) +
  geom_histogram(bins = 30, color = "black") +
  labs(x="Brewery Rating", y="# of Ratings") +
  ggtitle("Histogram of Ratings by Brewery")

# Plot the log distribution of # of Ratings per Beer Style
beer_style_data %>%
  ggplot(aes(x = s_numrtg)) +
  geom_histogram(bins = 50, color = "black") +
  scale_x_log10() +
  labs(x="# of Ratings (log10 scale)", y="# of Beers") +
  ggtitle("Distribution - # of Ratings by Brewery")

# Display graph of Average Rating by all Beer Styles
train_set %>%
  group_by(beer_style_id) %>%
  summarize(avg = mean(rating)) %>%
  ggplot(aes(x = as.numeric(reorder(beer_style_id, avg)), y = avg)) +
  geom_point() +
  geom_smooth( aes(x = as.numeric(reorder(beer_style_id, avg)), y = avg),
               method = 'lm', formula = y ~ poly(x, 4), se = TRUE) +
  theme(axis.text.x=element_blank()) +
  labs(x="Beer Style", y="Average Rating") +
  ggtitle("Mean Rating by Beer Style")

```

# Recommender System Methodology

As mentioned in the Introduction, this project will exapand on the previous MovieLens project and produce models from four different recommendation systems.

- Regularized Linear Regression
- XGB â€“ Extreme Parallel Tree Boosting (xgBoost)
- RECO - Recommender w/ Matrix Factorization (recosystem)
- Salford Predictive Modeler v8.0


## Standard & Regularized Linear Regression

As the first approach, we will build and test a Regularized Linear Regression model.  As previously seen, Regularization improves the results in all models, so we will skip doing Standard Linear Regression and only build Regularized Linear Regression models.  This will be done by successively adding mean distributions based upon each of the following variables in the dataset:

- Rating (mu) - Starting with an overall average of all ratings (mu)
- Beer (b_i) - Adding the Average rating of each individual Movie
- User (b_u) - Adding the Average rating of each individual User
- Style (s_u) - Adding the average for each unique Beer Style
- Brewery (brw_u) - Adding the average of each rating by Brewery

### Linear Regression Model Creation
#### Simple Overall Average

Create and Display the RMSE results of using the overall Average (mu) of all ratings:

```{r}
#*******************************************************************************
# We're only using this to get the absolute WORST RMSE that we're trying to beat
#*******************************************************************************
# Linear Regression - Using just the overall average
#   Yu,i = mu
#*******************************************************************************

# Calculate the overall average rating
mu <- mean(train_set$rating)
mu

# Evaluate the performance of simply guessing the overall average
rmse <- RMSE(test_set$rating, mu)

# Save the RMSE result to display later
LR_rmse_results <- tibble(Method = "LR: Base Mean Model",
                          RMSE = rmse)
kable(LR_rmse_results) %>%
  kable_styling(full_width = FALSE, position = "left")
```

Using just the overall mean (mu), we do surprisingly well, but the goal now is to do better than __0.7159839__

#### Beer Effect

To enhance the LR model, we will take a look at the rating for each Beer.  Plotting the distribution of Beer ratings (below) shows that ratings are not uniformly distributed and that the average of each beer should add predictive capabilities.

We will add a term to include the average Movie rating to the general simple mean, thereby adding the "Beer Effect:"

```{r}
#*******************************************************************************
#*******************************************************************************
#***********    Regularized Linear Regression - Model Building    **************
#*******************************************************************************
#*******************************************************************************

# Since we've previously seen that Regularized Linear Regression almost always
# outperformes Linear Regression, we are only going to use Penalized Least Squares

#*******************************************************************************
# Regularized Linear Regression - Base Average Model + Beer Effect:
#   Yu,i = mu + Beer_reg_avgs$b_i
#*******************************************************************************

# Find the best RMSE across range of Lambdas
lambdas <- seq(1, 3, 0.25)
rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set$rating)
  b_i <- train_set %>%
    group_by(beer_id) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  predicted_ratings <-
    test_set %>%
    left_join(b_i, by = "beer_id") %>%
    mutate(pred = mu + b_i) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
})
qplot(lambdas, rmses)

# Lambda resulting in best fit / lowest RMSE
lambda <- lambdas[which.min(rmses)]
lambda

LR_rmse_results <- bind_rows(LR_rmse_results,
                             tibble(Method="Reg LR: Mean + Beer Effect Model",
                                    RMSE = min(rmses)))
kable(LR_rmse_results) %>%
  kable_styling(full_width = FALSE, position = "left")
```

Adding the Beer average shows improvement, but we have to be able to do better.


#### User Effect

Next, we will add the "User Effect" by taking the average rating of each user and combining it with the current Mean + Beer model:

```{r}
#*******************************************************************************
# Regularized Linear Regression - Mean + Beer Effect + User Effect:
#   Yu,i = mu + b_i + b_u
#*******************************************************************************

# Find the best RMSE across range of Lambdas
lambdas <- seq(2.5, 3.5, 0.1)
rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set$rating)
  b_i <- train_set %>%
    group_by(beer_id) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  b_u <- train_set %>%
    left_join(b_i, by="beer_id") %>%
    group_by(user_id) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  predicted_ratings <-
    test_set %>%
    left_join(b_i, by = "beer_id") %>%
    left_join(b_u, by = "user_id") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
})
qplot(lambdas, rmses)

# Lambda resulting in best fit / lowest RMSE
lambda <- lambdas[which.min(rmses)]
lambda

LR_rmse_results <- bind_rows(LR_rmse_results,
                             tibble(Method="Reg LR: Mean + Beer + User Effect Model",
                                    RMSE = min(rmses)))
kable(LR_rmse_results) %>%
  kable_styling(full_width = FALSE, position = "left")
```


#### Beer Style Effect

As prviously seen, the beer styles get appreciably different ratings.

Adding in the Style Effect:

```{r}
#*******************************************************************************
# Regularized Linear Regression - Mean + Beer + User + Style Effect:
#   Yu,i = mu + b_i + b_u + s_u
#*******************************************************************************

# Find best Lambda to minimize RMSE
lambdas <- seq(2.8, 3.6, 0.1)
rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set$rating)
  b_i <- train_set %>%
    group_by(beer_id) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  b_u <- train_set %>%
    left_join(b_i, by="beer_id") %>%
    group_by(user_id) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  s_u <- train_set %>%
    left_join(b_i, by="beer_id") %>%
    left_join(b_u, by="user_id") %>%
    group_by(beer_style_id) %>%
    summarize(s_u = sum(rating - b_i - b_u - mu)/(n()+l))
  predicted_ratings <-
    test_set %>%
    left_join(b_i, by = "beer_id") %>%
    left_join(b_u, by = "user_id") %>%
    left_join(s_u, by = "beer_style_id") %>%
    mutate(pred = mu + b_i + b_u + s_u) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
})
qplot(lambdas, rmses)

lambda <- lambdas[which.min(rmses)]
lambda

LR_rmse_results <- bind_rows(LR_rmse_results,
                             tibble(Method="Reg LR: Mean + Beer + User + Style Effect Model",
                                    RMSE = min(rmses)))
kable(LR_rmse_results) %>%
  kable_styling(full_width = FALSE, position = "left")

```

Adding the Style Effect further improves the RMSE, but not by much.


#### Brewery Effect

Finally, we will add in the effect for each Brewery:

```{r}
#*******************************************************************************
# Regularized Linear Regression - Mean + Beer + User + Style + Brewery Effect:
#   Yu,i = mu + b_i + b_u + s_u + brw_u
#*******************************************************************************

# Find best Lambda to minimize RMSE
lambdas <- seq(3, 6, 0.1)
rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set$rating)
  b_i <- train_set %>%
    group_by(beer_id) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  b_u <- train_set %>%
    left_join(b_i, by="beer_id") %>%
    group_by(user_id) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  s_u <- train_set %>%
    left_join(b_i, by="beer_id") %>%
    left_join(b_u, by="user_id") %>%
    group_by(beer_style_id) %>%
    summarize(s_u = sum(rating - b_i - b_u - mu)/(n()+l))
  brw_u <- train_set %>%
    left_join(b_i, by="beer_id") %>%
    left_join(b_u, by="user_id") %>%
    left_join(s_u, by="beer_style_id") %>%
    group_by(brewery_id) %>%
    summarize(brw_u = sum(rating - b_i - b_u -s_u - mu)/(n()+l))
  predicted_ratings <-
    test_set %>%
    left_join(b_i, by = "beer_id") %>%
    left_join(b_u, by = "user_id") %>%
    left_join(s_u, by = "beer_style_id") %>%
    left_join(brw_u, by = "brewery_id") %>%
    mutate(pred = mu + b_i + b_u + s_u + brw_u) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
})
qplot(lambdas, rmses)

lambda <- lambdas[which.min(rmses)]
lambda
min(rmses)

LR_rmse_results <- bind_rows(LR_rmse_results,
                             tibble(Method="Reg LR: Mean + Beer + User + Style + Brewery Effect Model",
                                    RMSE = min(rmses)))
kable(LR_rmse_results) %>%
  kable_styling(full_width = FALSE, position = "left")
```

Adding the final Brewery Effect further lowers the RMSE value.


As seen from the above results, Regularizing using Penalized Least Squares improves every version of the model and gives the final "Best RMSE" of __0.5992131__


```{r}
# Clean up Environment
rm(lambda, lambdas, rmses, mu, beer)
```

## XGB â€“ Extreme Parallel Tree Boosting (xgBoost)

Since its introduction in 2015, xgBoost, along with Deep Neural Networks, have largely dominated the winning solutions in Kaggle competitions.

xgBoost (Extreme Gradient Boosting) is an R library optimized for tree boosting. Its utilization of multi-threading and regularization deliver very fast (when compared to previous Random Forest methods) and accurate predictions.  xgBoost is similar to other gradient boosting frameworks but focused on parallel computation on a single machine along with efficient memory usage.

xgBoost is also very flexible, implementing both linear models and tree learning algorithms.  It also supports many different objective functions, including regression, classification and ranking.

And unlike the poor results on the MovieLens data..  xgb proves its worth on the Beer data!

### xgBoost Data Preparation

xgBoost only works with numeric vectors.

#### Create Training and Test Datasets

```{r}
#*******************************************************************************
#*******************************************************************************
#***********    XGB - Extreme Parallel Tree Boosting (xgBoost)    **************
#*******************************************************************************
#*******************************************************************************

# Load required libraries
if (!require(xgboost)) install.packages('xgboost')
library(xgboost)

if (!require(data.table)) install.packages('data.table')
library(data.table)

# Create new Training and Test datasets
# xgBoost requires that all values to be numeric

train_set <- beer_train %>% select(-one_of("brewery_name", "timestamp", "beer_style",
                                           "beer_name"))
test_set  <- beer_test  %>% select(-one_of("brewery_name", "timestamp", "beer_style",
                                           "beer_name"))

```


#### Additional Beer & User Derived Variables


```{r}
# Merge User, Beer, Brewery, and Beer Style derived fields into the Training and Test datasets
train_set <- train_set %>% left_join(user_data,       by = "user_id")
train_set <- train_set %>% left_join(beer_data,       by = "beer_id")
train_set <- train_set %>% left_join(brewery_data,    by = "brewery_id")
train_set <- train_set %>% left_join(beer_style_data, by = "beer_style_id")

test_set <- test_set %>% left_join(user_data,       by = "user_id")
test_set <- test_set %>% left_join(beer_data,       by = "beer_id")
test_set <- test_set %>% left_join(brewery_data,    by = "brewery_id")
test_set <- test_set %>% left_join(beer_style_data, by = "beer_style_id")

```

Now that the Training & Test datasets have all of the additional derived variables included, the sets must be split to provide the data and the target "label" data tables.  xgBoost uses its own Data Matrix format called xgb.DMatrix:

```{r}
# xgBoost requires that the target variable, referred to as the "label" (rating) be in a separate data table
# Here we create a Label and Data for the Training and Testing sets
# We are also dropping the Genres variable since it isn't numeric and no longer needed due to the One-Hot encoding added earlier

train_data  <- train_set %>% select(-one_of("rating")) %>% as.data.table()
train_label <- train_set %>% select("rating")          %>% as.data.table()
test_data   <- test_set  %>% select(-one_of("rating")) %>% as.data.table()
test_label  <- test_set  %>% select("rating")          %>% as.data.table()

# Create the xgBoost Training & Testing matricies
train_matrix = xgb.DMatrix(as.matrix(train_data), label=as.matrix(train_label))
test_matrix = xgb.DMatrix(as.matrix(test_data), label=as.matrix(test_label))

```

### xgBoost - XGB Model Creation

xgBoost allows the creation of both Linear, Tree, and Mixed Linear & Tree based models.  xgBoost can also create Classification trees (binary and multi-class) by changing the objective function.  While xgBoost is very fast when compared to other methods, the training times can be excessive.

Because the training times are acceptable for this relatively small dataset, we will be creating four small models:

- XGB Linear 50 Boost Rnds
- XGB Linear 1000 Boost Rnds
- XGB Mixed Tree 5 Boost Rnds

```{r}
#**********************************************************************************************
# Model: XGB Linear 50 Boost Rnds
#**********************************************************************************************
# Set paramaters for most basic Linear tree
# These settings should train in less than a minute with 50 Boosting Rounds
xgb_params <- list(booster = "gblinear",      # Linear boosting alg
                   objective = "reg:linear",  # Linear regression (default)
                   eval_metric = "rmse",      # rmse as objective function
                   verbosity = 3,             # Highest verbosr level - shows all debug info
                   silent = 0)                # Not silent

# Train the XGB tree using the currently set xgb_params
start_time <- Sys.time()                    # Record start time

set.seed(1, sample.kind = "Rounding")
xgb_model <- xgboost(params = xgb_params,
                     data = train_matrix,
                     nrounds = 50,          # Maximum number of Boosting Rounds
                     nthread = 1,           # Must be set to 1 to get reproducible results
                     verbose = 2)           # Display pogress during Training

finish_time <- Sys.time()                   # Record finish time
finish_time - start_time                    # Display total training time

# Use the trained model to predict the Test dataset
test_pred <- as.data.frame(predict(xgb_model , newdata = test_matrix))

# Calculate the RMSE of the Predictions
rmse <- RMSE(test_label$rating, test_pred$`predict(xgb_model, newdata = test_matrix)`)

XGB_rmse_results <- tibble(Method="XGB Linear 50 Boost Rnds",
                           RMSE = rmse,
                           Train_Time = paste(as.character(round(as.numeric(finish_time - start_time, units="secs"), 2)), "secs"))
XGB_rmse_results %>% knitr::kable()


```

```{r}
#**********************************************************************************************
# Model: XGB Linear 1000 Boost Rnds
#**********************************************************************************************

xgb_params <- list(booster = "gblinear",      # Linear boosting alg
                   objective = "reg:linear",  # Linear regression (default)
                   eval_metric = "rmse",      # rmse as objective function
                   verbosity = 3,             # Highest verbosr level - shows all debug info
                   silent = 0)                # Not silent

# Train the XGB tree using the currently set xgb_params
start_time <- Sys.time()                    # Record start time

set.seed(1, sample.kind = "Rounding")
xgb_model <- xgboost(params = xgb_params,
                     data = train_matrix,
                     nrounds = 1000,        # Maximum number of Boosting Rounds
                     nthread = 1,           # Must be set to 1 to get reproducible results
                     verbose = 2)           # Display pogress during Training

finish_time <- Sys.time()                   # Record finish time
finish_time - start_time                    # Display total training time

# Use the trained model to predict the Test dataset
test_pred <- as.data.frame(predict(xgb_model , newdata = test_matrix))

# Calculate the RMSE of the Predictions
rmse <- RMSE(test_label$rating, test_pred$`predict(xgb_model, newdata = test_matrix)`)

XGB_rmse_results <- bind_rows(XGB_rmse_results,
                              tibble(Method="XGB Linear 1000 Boost Rnds",
                                     RMSE = rmse,
                                     Train_Time = paste(as.character(round(as.numeric(finish_time - start_time, units="secs"), 2)), "secs")))

kable(XGB_rmse_results) %>%
  kable_styling(full_width = FALSE, position = "left")


```


```{r}
#**********************************************************************************************
# Model: XGB Mixed Tree 5 Boost Rnds
#**********************************************************************************************

xgb_params <- list(booster = "gbtree",
                   objective = "reg:linear",
                   colsample_bynode = 0.8,
                   learning_rate = 1,
                   max_depth = 10,
                   num_parallel_tree = 25,
                   subsample = 0.8,
                   verbosity = 3,
                   silent = 0)

# Train the XGB tree using the currently set xgb_params
start_time <- Sys.time()                    # Record start time

set.seed(1, sample.kind = "Rounding")
xgb_model <- xgboost(params = xgb_params,
                     data = train_matrix,
                     nrounds = 5,           # Maximum number of Boosting Rounds
                     nthread = 1,           # Must be set to 1 to get reproducible results
                     verbose = 2)           # Display pogress during Training

finish_time <- Sys.time()                   # Record finish time
finish_time - start_time                    # Display total training time

# Use the trained model to predict the Test dataset
test_pred <- as.data.frame(predict(xgb_model , newdata = test_matrix))

# Calculate the RMSE of the Predictions
rmse <- RMSE(test_label$rating, test_pred$`predict(xgb_model, newdata = test_matrix)`)

XGB_rmse_results <- bind_rows(XGB_rmse_results,
                              tibble(Method="XGB Mixed Tree 5 Boost Rnds",
                                     RMSE = rmse,
                                     Train_Time = paste(as.character(round(as.numeric(finish_time - start_time, units="secs"), 2)), "secs")))

kable(XGB_rmse_results) %>%
  kable_styling(full_width = FALSE, position = "left")

```


#### xgBoost - Observations and Additional Comments

Despite the ability to use a full Training set including many additional derived parameters, the xgBoost models were only able to outperform the Linear Regression Models when using 50 Boosting 


Clean up the Environment before moving to the next model.

```{r}
rm(finish_time, importance_matrix, movie_data, rmse, start_time, test_data, test_label,
   test_matrix, test_pred, test_set, train_data, train_label, train_matrix, train_set,
   user_data, xgb_model, xgb_params)
```

## Recosystem - Matrix Factorization w/ Parallel Stochastic Gradient Descent

While we had success with xgBoost, there are several "Recommender" packages available in the R ecosystem focused specifically on Collaborative Filtering.  Within the Recommender system, the main task is to predict unknown entries in a rating matrix composed of Users vs Items.

One technique used to solve the recommender problem is matrix factorization where the rating matrix is successively approximated by the product of two matrices of lower dimensions.

Recosystem is a wrapper for the open-source LibFM library that implements optimized and parallelized matrix factorization. The recosystem library also provides easy to use functions to perform parameter tuning across a grid of parameter options, training against an in-memory or on-disk dataset using the tuned parameters, and prediction of results against a test set.

### RECO - Data Preparation

Recosystem only uses three variables:

- __Rating (rating):__ Rating score provided by User for Movie
- __User (user_id):__ User ID associated with the Rating
- __Beer (beer_id):__ Beer ID associated with the Rating

#### Create Training and Test Datasets

Given the few required variables, the datasets are quite small and should easily fit into system memory.  We will only be keeping the __rating__, __user_id__, and __beer_id__ columns:

```{r}
#*******************************************************************************
#*******************************************************************************
#** Recosystem - Matrix Factorization w/ Parallel Stochastic Gradient Descent **
#*******************************************************************************
#*******************************************************************************

# Load required libraries
if (!require(recosystem)) install.packages('recosystem')
library(recosystem)

#************************************************************************************
# Recosystem - Matrix Factorization with Parallel Stochastic Gradient Descent
#************************************************************************************

# Create new Traing and Test datasets. Recosystem only uses three columns: Rating, user_id, and beer_id
train_set <- beer_train %>% select("user_id","beer_id", "rating") %>% as.matrix()
test_set <-  beer_test %>% select("user_id","beer_id", "rating") %>% as.matrix()

```
#### Create the Recosystem Object

```{r}
# Create the Reco Recommender object
r = Reco()


```

#### Create the Tuning Parameter Grid

One very nice feature of Recosystem, besides it's speed, is that tuning the training parameters is very easy and the overall best performing tuned parameters are stored directly in the Reco object making calling them against the Training set simple.

Below we will create a small Tuning grid as an example.  This tuning grid only tunes between 2 values of __dim__ (10 & 20).

_NOTE: We can use nthread = X in the parameter list to define the number of threads to use. If omitted, Recosystem will try to use the maximum available.  I used nthread = 8_

```{r}
# One very nice feature of Reco, besides it's speed, is that tuning training set is very easy and the
# overall best performing tuned paramaters are stored directly in the Reco object.

# Here we will create some Tuning Grids

#***********************************************************************************
# First Tuning Round - Takes around 10 minutes with 8 Threads
#***********************************************************************************
opts_list = list(dim      = c(10, 20),         # Number of Latent Features
                 costp_l1 = c(0, 0.1),         # L1 regularization cost for User factors
                 costp_l2 = c(0.01, 0.1),      # L2 regularization cost for User factors
                 costq_l1 = c(0, 0.1),         # L1 regularization cost for Beer factors
                 costq_l2 = c(0.01, 0.1),      # L2 regularization cost for Beer factors
                 lrate    = c(0.01, 0.1),      # Learning Rate - Aprox step size in Gradient Descent
                 niter    = 10,                # Number of Iterations for Training (Not used in Tuning)
                 nfolds   = 5,                 # Number of Folds for CV in Tuning
                 verbose  = FALSE,             # Don't Show Progress
                 nthread  = 8)        #!!! Can be set to higher values for Tuning, but MUST be set to 1
                                      #    for Training or the results are not reproducible
# RESULTS
# dim      20       
# costp_l1 0        
# costp_l2 0.01     
# costq_l1 0.1      
# costq_l2 0.1      
# lrate    0.1 

#***********************************************************************************

```

#### Tune the Model using the Parameter Grid

As noted, if you want, you can use the __nthread__ paramater, but it can't be used as a Training parameter.  It requires additional investigation, but __nthread__ must be set to 1 during Training or the results cannot be reproduced.  There must be some randomization in the parallelization that does not rely on setting the Seed value.

Next, we will tune the model given the parameter grid above:

```{r}
#***********************************************************************************
# TUNE the Mode
#***********************************************************************************

start <- Sys.time()

set.seed(1, sample.kind = "Rounding")
opts_tune = r$tune(data_memory(train_set[, 1], train_set[, 2], train_set[, 3]),
                   opts = opts_list)

finish <- Sys.time()
tune_time <- finish - start
tune_time
opts_tune$min


```

We see that the Tuned options chose 20 for the __dim__ variable.  Many iterations were performed to optimize the other model parameters.  For the sake of time, we won't perform additional optimizations, but we will look later at the results of using Tuning and 5-fold Cross-Validation in order to find a semi-optimal number of Factors ( __dim__ )

#### Train the Model using the Best Parameters

Once the parameters have been optimized, we Train the model:

```{r}
#***********************************************************************************
# TRAIN the Model over 50 Iterations
#***********************************************************************************
start <- Sys.time()

set.seed(1, sample.kind = "Rounding")
r$train(data_memory(train_set[, 1], train_set[, 2], train_set[, 3]),
        opts = c(opts_tune$min,
                 niter    = 50,     # Train over 50 Iterations
                 nthread = 1))      # Must be set to 1 for training

finish <- Sys.time()
train_time <- finish - start
train_time

```

#### Predict the Test Set

After training, we use the trained model to predict the Ratings in the Test Set and calculate the RMSE:

```{r}
#***********************************************************************************
# PREDICT the Test Ratings
#***********************************************************************************
start <- Sys.time()

pred <- r$predict(data_memory(test_set[, 1], test_set[, 2], test_set[, 3]), out_memory())

finish <- Sys.time()
pred_time <- finish - start
pred_time

```

#### Calculate the RMSE

```{r}
#***********************************************************************************
# Calculate the RMSE
#***********************************************************************************

rmse <- RMSE(test_set[,3],pred)

RECO_rmse_results <- tibble(Method="V1 - 20 x 50",
                            RMSE = rmse,
                            Train_Time = round(as.numeric(train_time, units="mins"), 2))
kable(RECO_rmse_results) %>%
  kable_styling(full_width = FALSE, position = "left")

```

As mentioned previously, we will now look at the 5-Fold Cross Validation results using the Tuning function to predict the RMSE across values of the __dim__ parameter between 10 and 100:

```{r}
#***********************************************************************************
# Tune the DIM parameter - FOR 10-100 and then 10-20 and Graph
# !!!!  NOTE !!!!  THESE WILL TAKE A LONG TIME TO RUN
#***********************************************************************************
# Tune & Graph Latent Factors (dim) from 10 to 60 by 10
#***********************************************************************************

opts_list = list(dim      = c(seq(10, 60, 10)),    # Number of Latent Features
                 costp_l1 = c(0),       # L1 regularization cost for User factors
                 costp_l2 = c(0.01),    # L2 regularization cost for User factors
                 costq_l1 = c(0.1),     # L1 regularization cost for Beer factors
                 costq_l2 = c(0.1),     # L2 regularization cost for Beer factors
                 lrate    = c(0.1),     # Learning Rate - Aprox step size in Gradient Descent
                 nfold    = 5,          # Number of folds for Cross Validation
                 nthread  = 8,          # Set Number of Threads to 1 *** Required to get Reproducible Results ***
                 niter    = 50,         # Number of Iterations
                 verbose  = FALSE)      # Don't Show Fold Details

start <- Sys.time()
set.seed(1, sample.kind = "Rounding")
opts_tune <- r$tune(data_memory(train_set[, 1], train_set[, 2], train_set[, 3]),
                    opts = opts_list)
finish <- Sys.time()
tune_time <- finish - start

opts_tune$min

opts_tune$res %>%
  ggplot(aes(dim, loss_fun)) +
  geom_point() +
  geom_smooth(method="loess") +
  labs(x="Latent Factors (dim)", y="RMSE") +
  ggtitle("Latent Factors vs RMSE")

RECO_CV_10_100 <- opts_tune


```

We see that the predicted minimum is somewhere between 10 & 30, so we'll re-run the Tuning between 10 and 40 to get a better look:

```{r}

#***********************************************************************************
# Tune & Graph Latent Factors (dim) from 10 to 30
#***********************************************************************************

opts_list = list(dim      = c(seq(10, 30, 2)),    # Number of Latent Features
                 costp_l1 = c(0),       # L1 regularization cost for User factors
                 costp_l2 = c(0.01),    # L2 regularization cost for User factors
                 costq_l1 = c(0.1),     # L1 regularization cost for Beer factors
                 costq_l2 = c(0.1),     # L2 regularization cost for Beer factors
                 lrate    = c(0.1),     # Learning Rate - Aprox step size in Gradient Descent
                 nfold    = 5,          # Number of folds for Cross Validation
                 nthread  = 8,          # Set Number of Threads to 1 *** Required to get Reproducible Results ***
                 niter    = 50,         # Number of Iterations
                 verbose  = FALSE)      # Don't Show Fold Details

start <- Sys.time()
set.seed(1, sample.kind = "Rounding")
opts_tune <- r$tune(data_memory(train_set[, 1], train_set[, 2], train_set[, 3]),
                    opts = opts_list)
finish <- Sys.time()
tune_time <- finish - start

opts_tune$min

opts_tune$res %>%
  ggplot(aes(dim, loss_fun)) +
  geom_point() +
  geom_smooth(method="loess") +
  labs(x="Latent Factors (dim)", y="RMSE") +
  ggtitle("Latent Factors vs RMSE")

RECO_CV_15_30 <- opts_tune

```

From this, __dim = 16__ gives the smallest RMSE, so we will re-Train the model at __dim = 16__ and see how it compares to the current RMSE result:

```{r}
#***********************************************************************************
# Train "Optimal" RMSE - (dim = 16)  V3 - 16 x 50
#***********************************************************************************
opts_list = list(dim      = c(16),      # Number of Latent Features
                 costp_l1 = c(0),       # L1 regularization cost for User factors
                 costp_l2 = c(0.01),    # L2 regularization cost for User factors
                 costq_l1 = c(0.1),     # L1 regularization cost for Beer factors
                 costq_l2 = c(0.1),     # L2 regularization cost for Beer factors
                 lrate    = c(0.1),     # Learning Rate - Aprox step size in Gradient Descent
                 nfold    = 5,          # Number of folds for Cross Validation
                 nthread  = 1,          # Set Number of Threads to 1 *** Required to get Reproducible Results ***
                 niter    = 50,         # Number of Iterations
                 verbose  = FALSE)      # Don't Show Fold Details

start <- Sys.time()

set.seed(1, sample.kind = "Rounding")
r$train(data_memory(train_set[, 1], train_set[, 2], train_set[, 3]),
        opts = c(opts_list,
                 nthread = 1))

finish <- Sys.time()
train_time <- finish - start

start <- Sys.time()

pred <- r$predict(data_memory(test_set[, 1], test_set[, 2], test_set[, 3]), out_memory())
finish <- Sys.time()

pred_time <- finish - start
pred_time

rmse <- RMSE(test_set[,3],pred)

RECO_rmse_results <- bind_rows(RECO_rmse_results,
                               tibble(Method="V2 - 16 x 50 (Optimal DIM)",
                                      RMSE = rmse,
                                      Train_Time = round(as.numeric(train_time, units="mins"), 2)))
kable(RECO_rmse_results) %>%
  kable_styling(full_width = FALSE, position = "left")

#***********************************************************************************


```

Here the model with __dim = 16__ produced the minimal RMSE in the Cross-Validation Tuning.  As a test, we will create a very Large model with __dim = 1000__ and train it over 100 iterations to see what the actual RMSE is on the Test set.  The code is provided below but will not be run because it takes almost 1 Hour to complete.  The resulting RMSE will simply be added to the current results table.

```{r}
#***********************************************************************************
# Tune & Graph Latent Factors (dim) from 100 to 1000 by 100
#***********************************************************************************

opts_list = list(dim      = c(seq(100, 1000, 100)),    # Number of Latent Features
                 costp_l1 = c(0),       # L1 regularization cost for User factors
                 costp_l2 = c(0.01),    # L2 regularization cost for User factors
                 costq_l1 = c(0.1),     # L1 regularization cost for Beer factors
                 costq_l2 = c(0.1),     # L2 regularization cost for Beer factors
                 lrate    = c(0.1),     # Learning Rate - Aprox step size in Gradient Descent
                 nfold    = 5,          # Number of folds for Cross Validation
                 nthread  = 8,          # Set Number of Threads to 1 *** Required to get Reproducible Results ***
                 niter    = 50,         # Number of Iterations
                 verbose  = FALSE)      # Don't Show Fold Details

start <- Sys.time()
set.seed(1, sample.kind = "Rounding")
opts_tune <- r$tune(data_memory(train_set[, 1], train_set[, 2], train_set[, 3]),
                    opts = opts_list)
finish <- Sys.time()
tune_time <- finish - start

opts_tune$min

opts_tune$res %>%
  ggplot(aes(dim, loss_fun)) +
  geom_point() +
  geom_smooth(method="loess") +
  labs(x="Latent Factors (dim)", y="RMSE") +
  ggtitle("Latent Factors vs RMSE")

RECO_CV_100_1000 <- opts_tune


#***********************************************************************************
# Train "Large" - V4 - 1000 x 100
#***********************************************************************************
opts_list = list(dim      = c(1000),    # Number of Latent Features
                 costp_l1 = c(0),       # L1 regularization cost for User factors
                 costp_l2 = c(0.01),    # L2 regularization cost for User factors
                 costq_l1 = c(0.1),     # L1 regularization cost for Beer factors
                 costq_l2 = c(0.1),     # L2 regularization cost for Beer factors
                 lrate    = c(0.1),     # Learning Rate - Aprox step size in Gradient Descent
                 nfold    = 5,          # Number of folds for Cross Validation
                 nthread  = 1,          # Set Number of Threads to 1 *** Required to get Reproducible Results ***
                 niter    = 100,        # Number of Iterations
                 verbose  = FALSE)      # Don't Show Fold Details

start <- Sys.time()

set.seed(1, sample.kind = "Rounding")
r$train(data_memory(train_set[, 1], train_set[, 2], train_set[, 3]),
        opts = c(opts_list,
                 nthread = 1))

finish <- Sys.time()
train_time <- finish - start

start <- Sys.time()

pred <- r$predict(data_memory(test_set[, 1], test_set[, 2], test_set[, 3]), out_memory())
finish <- Sys.time()

pred_time <- finish - start
pred_time

rmse <- RMSE(test_set[,3],pred)

RECO_rmse_results <- bind_rows(RECO_rmse_results,
                               tibble(Method="V3 - 1000 x 100",
                                      RMSE = rmse,
                                      Train_Time = round(as.numeric(train_time, units="mins"), 2)))

kable(RECO_rmse_results) %>%
  kable_styling(full_width = FALSE, position = "left")

#***********************************************************************************

```

These results show that a very large __dim__ value for Latent Factors still delivers an increase in performance.  It is unknown what the limit is for the number of Latent Factors.


## Salford Predictive Modeler v8.0 - CART, MARS, and a Whole Lot of Other Models

[Salford Systems](https://www.salford-systems.com/), recently acquired by MiniTab, has produced the Predictive Modeling program and enjoyed very impressive results in many Data Science and Machine Learning comptetitions.  It isn't R, but it also doesn't have any problem modeling large data sets.  Out of curiosity, I decided to train some models and compare the results.

You can download a trial version of the software from their website... they want to charge $13K for the software, but given the results, it doesn't appear to be worth the money these days.

```{r}

SPM_rmse_results <- tibble(Method="Random Forests:",
                                    RMSE = 0.62616)

SPM_rmse_results <- bind_rows(SPM_rmse_results,
                              tibble(Method="CART - Ensemble:",
                                     RMSE = 0.62202))

SPM_rmse_results <- bind_rows(SPM_rmse_results,
                              tibble(Method="CART:",
                                     RMSE = 0.62089))

SPM_rmse_results <- bind_rows(SPM_rmse_results,
                              tibble(Method="MARS:",
                                     RMSE = 0.61482))

SPM_rmse_results <- bind_rows(SPM_rmse_results,
                              tibble(Method="Regression:",
                                     RMSE = 0.61239))

SPM_rmse_results <- bind_rows(SPM_rmse_results,
                              tibble(Method="GPS - Generalized Lasso:",
                                     RMSE = 0.59585))

kable(SPM_rmse_results) %>%
  kable_styling(full_width = FALSE, position = "left")

```

# Results

Below are the tabulated results of all methods.

- Standard & Regularized Linear Regression
```{r}
kable(LR_rmse_results) %>%
  kable_styling(full_width = FALSE, position = "left")

```

- XGB â€“ Extreme Parallel Tree Boosting (xgBoost)
```{r}
kable(XGB_rmse_results) %>%
  kable_styling(full_width = FALSE, position = "left")

```

- RECO - Recommender w/ Matrix Factorization (recosystem)
```{r}
kable(RECO_rmse_results) %>%
  kable_styling(full_width = FALSE, position = "left")
```

- Salford Predictive Modeler v8.0
```{r}
kable(SPM_rmse_results) %>%
  kable_styling(full_width = FALSE, position = "left")
```


# Conclusion

This Capstone Individual project has applied four different techniques toward the development of a Beer Recommendation algorithm.

It is interesting to see that the Regularized Linear Regression performed better than both xgBoost and recosystem. At the end of the day, Salford won out, but it was close.


